import streamlit as st
import pandas as pd
import requests
from difflib import get_close_matches
import folium
from streamlit_folium import st_folium
import matplotlib
import matplotlib.cm as cm
import matplotlib.colors as mcolors

# ========= ä½ çš„ eBird API Key =========
EBIRD_API_KEY = "3g5voge8rcai"   


# -------------------- æœ€è¿‘è§‚æµ‹ï¼ˆæŒ‰åœ°åŒºï¼‰ --------------------
def fetch_ebird_data(region="US-IL"):
    url = f"https://api.ebird.org/v2/data/obs/{region}/recent"
    headers = {"X-eBirdApiToken": EBIRD_API_KEY}

    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        st.error(f"Error {res.status_code}: {res.text}")
        return pd.DataFrame()

    return pd.DataFrame(res.json())


# -------------------- åª’ä½“ API: æ ¹æ® taxonCode æŠ“ç…§ç‰‡ --------------------
def fetch_bird_photo(taxon_code: str):
    url = f"https://api.ebird.org/v2/media/catalog?taxonCode={taxon_code}&mediaType=photo"
    headers = {"X-eBirdApiToken": EBIRD_API_KEY}

    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        return None

    items = res.json()
    if not items:
        return None

    # æ‹¿ç¬¬ä¸€å¼ 
    return items[0].get("mediaUrl")


# -------------------- å…¨ç¾æœ€è¿‘ 30 å¤©è¯¥é¸Ÿçš„è§‚æµ‹ --------------------
def fetch_us_recent_for_species(taxon_code: str):
    url = f"https://api.ebird.org/v2/data/obs/US/recent/{taxon_code}"
    headers = {"X-eBirdApiToken": EBIRD_API_KEY}

    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        st.error(f"Failureï¼š{res.status_code} {res.text}")
        return pd.DataFrame()

    return pd.DataFrame(res.json())


# -------------------- æ¨¡ç³ŠåŒ¹é…é¸Ÿå --------------------
def find_best_match(df: pd.DataFrame, user_input: str):
    names = df["comName"].dropna().unique().tolist()
    matches = get_close_matches(user_input, names, n=1, cutoff=0.3)
    return matches[0] if matches else None


# -------------------- æ ¹æ® DataFrame ç”»çƒ­åŠ›å›¾ï¼ˆæ—¶é—´æ¸å˜é¢œè‰²ï¼‰ --------------------
def build_heatmap(all_df: pd.DataFrame):
    # ç¡®ä¿å­—æ®µå­˜åœ¨
    needed_cols = {"lat", "lng", "obsDt", "comName"}
    if not needed_cols.issubset(all_df.columns):
        st.error(f"Missing required fields: {needed_cols - set(all_df.columns)}")
        return None

    # å¤„ç†æ—¶é—´
    all_df = all_df.copy()
    all_df["obsDt"] = pd.to_datetime(all_df["obsDt"], errors="coerce")
    all_df = all_df.dropna(subset=["obsDt", "lat", "lng"])

    if all_df.empty:
        st.warning("There are no valid observations for this species across the U.S. in the past 30 days.")
        return None

    # é¢œè‰²æ˜ å°„ï¼šè¶Šæ–°çš„æ—¥æœŸé¢œè‰²è¶Šåçº¢
    min_ts = all_df["obsDt"].min().timestamp()
    max_ts = all_df["obsDt"].max().timestamp()
    norm = mcolors.Normalize(vmin=min_ts, vmax=max_ts)
    cmap = cm.get_cmap("YlOrRd")

    def dt_to_color(dt):
        ts = dt.timestamp()
        return matplotlib.colors.to_hex(cmap(norm(ts)))

    all_df["color"] = all_df["obsDt"].apply(dt_to_color)

    # Folium åœ°å›¾
    m = folium.Map(
        location=[all_df["lat"].mean(), all_df["lng"].mean()],
        zoom_start=4,
        tiles="cartodb positron",
    )

    for _, r in all_df.iterrows():
        popup = f"""
        <b>{r.get('comName', '')}</b><br>
        Spotï¼š{r.get('locName', 'æœªçŸ¥')}<br>
        Countï¼š{r.get('howMany', 'N/A')}<br>
        Dateï¼š{r['obsDt'].strftime('%Y-%m-%d')}
        """
        folium.CircleMarker(
            location=[r["lat"], r["lng"]],
            radius=5,
            color=r["color"],
            fill=True,
            fill_opacity=0.7,
            popup=popup,
        ).add_to(m)

    return m


# =========================================================

st.set_page_config(page_title="eBird è§‚é¸ŸåŠ©æ‰‹", layout="wide")

st.title("ğŸ¦… eBird Bird Search + Photos + U.S. Heatmap")

# åˆå§‹åŒ– session_stateï¼Œç”¨æ¥â€œè®°ä½â€æ•°æ®ï¼Œé˜²æ­¢æŒ‰é’®åˆ‡æ¢æ—¶å†…å®¹æ¶ˆå¤±
if "region_df" not in st.session_state:
    st.session_state["region_df"] = pd.DataFrame()
if "heatmap_df" not in st.session_state:
    st.session_state["heatmap_df"] = pd.DataFrame()
if "heatmap_bird_name" not in st.session_state:
    st.session_state["heatmap_bird_name"] = None

# ---------- å·¦è¾¹ï¼šè®¾ç½®ã€åˆ—è¡¨ & æœç´¢ ----------
col1, col2 = st.columns([1, 1.2])

with col1:
    st.subheader("â‘  Fetch Recent Sightings by Region")

    region = st.text_input("Enter a region code (e.g., US-IL, default: US-IL)", "US-IL")

    if st.button("Fetch Bird Observation Data"):
        df_region = fetch_ebird_data(region)
        st.session_state["region_df"] = df_region  # å­˜èµ·æ¥
        if df_region.empty:
            st.warning("No data found.")
        else:
            st.success(f"Successfully fetched {len(df_region)} records.")

    if not st.session_state["region_df"].empty:
        st.dataframe(st.session_state["region_df"].head())

    st.markdown("---")
    st.subheader("â‘¡ Search Species, Show Photo & Generate Heatmap")

    user_bird = st.text_input("Enter a bird name (fuzzy match supported, e.g., â€˜sparrowâ€™, â€˜robinâ€™)")

    if st.button("Search & Generate Heatmap"):
        # ä¼˜å…ˆç”¨å·²ç»æ‹‰è¿‡çš„åœ°åŒº dfï¼Œæ²¡æœ‰å°±å†æ‹‰ä¸€æ¬¡
        df = st.session_state["region_df"]
        if df.empty:
            df = fetch_ebird_data(region)
            st.session_state["region_df"] = df

        if df.empty:
            st.warning("The dataset is empty. Please make sure the selected region has observation records.")
        else:
            best = find_best_match(df, user_bird)

            if not best:
                st.error("No matching bird name found.")
            else:
                st.success(f"Closest matchï¼š**{best}**")

                row = df[df["comName"] == best].iloc[0]
                taxon_code = row.get("taxonCode") or row.get("speciesCode")

                if not taxon_code:
                    st.error("This species has no taxonCode, so photos and heatmaps cannot be retrieved.")
                else:
                    # ç…§ç‰‡
                    img_url = fetch_bird_photo(taxon_code)
                    if img_url:
                        st.image(img_url, caption=best, use_container_width=True)
                    else:
                        st.write(f"ğŸ“·View on eBird: https://ebird.org/species/{taxon_code}")

                    st.write("### ğŸ“ A Recent Observation from This Region")
                    st.json(row.to_dict())

                    # å…¨ç¾ 30 å¤©è§‚æµ‹ï¼Œå­˜åˆ° session_stateï¼Œç”¨äºå³è¾¹çƒ­åŠ›å›¾
                    all_df = fetch_us_recent_for_species(taxon_code)
                    st.session_state["heatmap_df"] = all_df
                    st.session_state["heatmap_bird_name"] = best


# ---------- å³è¾¹ï¼šå…¨ç¾æœ€è¿‘ 30 å¤©çƒ­åŠ›å›¾ ----------
with col2:
    st.subheader("â‘¢ Nationwide Heatmap of Observations in the Past 30 Days")

    if (
        st.session_state["heatmap_df"] is None
        or st.session_state["heatmap_df"].empty
        or st.session_state["heatmap_bird_name"] is None
    ):
        st.info("ğŸ‘‰ Please search for a bird first to generate the heatmap data.")
    else:
        all_df = st.session_state["heatmap_df"]
        bird_name = st.session_state["heatmap_bird_name"]

        st.markdown(f"**Current Species:{bird_name}**  ï¼ˆPast 30 Days, U.S.ï¼‰")

        folium_map = build_heatmap(all_df)
        if folium_map is not None:
            st_data = st_folium(folium_map, width=800, height=550)




# =========================================================
# =============== â‘£ Migration Trend & Prediction ==========
# =========================================================

import numpy as np
import re

st.write("---")
st.subheader("â‘£ Migration Trend & Prediction (Past 30 Days â†’ Hotspots & Direction)")

# Only execute if heatmap data exists
if (
    "heatmap_df" in st.session_state
    and isinstance(st.session_state["heatmap_df"], pd.DataFrame)
    and not st.session_state["heatmap_df"].empty
):
    df_pred = st.session_state["heatmap_df"].copy()

    # ---- Date processing ----
    df_pred["obsDt"] = pd.to_datetime(df_pred["obsDt"], errors="coerce")
    df_pred = df_pred.dropna(subset=["obsDt", "lat", "lng", "locName"])
    df_pred = df_pred.sort_values("obsDt")

    # ============================================================
    # ========== â‘  Next 7 Days Hotspot Prediction (Cleaned) ======
    # ============================================================

    st.markdown("### ğŸŒ† Top Likely Hotspot Areas for the Next 7 Days")

    # ---------- Clean function (ZIP + City only) ----------
    def clean_loc_name(name: str):
        """
        Clean eBird location names:
        - Remove backyard/home/private locations
        - Extract ZIP code if present
        - Extract city name (usually second-to-last item)
        - Output format: ZIP â€“ City
        - If no ZIP: City only
        - If no city: return main part of location
        """
        if not isinstance(name, str):
            return None

        raw = name.strip()
        name_lower = raw.lower()

        # Remove private locations
        bad_keywords = [
            "yard", "backyard", "my yard", "front yard",
            "home", "my home", "house", "my house",
            "feeder", "garden", "patio", "my place"
        ]
        for kw in bad_keywords:
            if kw in name_lower:
                return None  # drop

        # Extract ZIP code
        zip_match = re.search(r"\b\d{5}\b", raw)
        zip_code = zip_match.group() if zip_match else None

        # Extract city
        parts = [p.strip() for p in raw.split(",") if len(p.strip()) > 0]
        city = None
        if len(parts) >= 2:
            candidate = parts[-2]
            if len(candidate) > 2:
                city = candidate

        # Output rules
        if zip_code and city:
            return f"{zip_code} â€“ {city}"

        if city:
            return city

        if zip_code:
            return zip_code

        return parts[0] if len(parts) >= 1 else raw


    if df_pred.empty:
        st.warning("Not enough data to predict hotspots.")
    else:
        # Apply cleaning logic
        df_pred["clean_loc"] = df_pred["locName"].apply(clean_loc_name)
        df_clean = df_pred.dropna(subset=["clean_loc"])

        if df_clean.empty:
            st.warning("No valid public hotspot locations after filtering.")
        else:
            vc = df_clean["clean_loc"].value_counts()

            top_areas = pd.DataFrame({
                "Area": vc.index,
                "Observations": vc.values
            })

            top_areas["Observations"] = pd.to_numeric(
                top_areas["Observations"], errors="coerce"
            ).fillna(0).astype(int)

            total_count = top_areas["Observations"].sum()
            top_areas["Probability"] = (
                top_areas["Observations"] / total_count
                if total_count > 0 else 0
            )

            st.write("ğŸ“ **Top 5 Likely Hotspot Areas (Cleaned & Filtered):**")
            st.dataframe(top_areas.head(5))

            st.info(
                "Private locations (backyard, home, feeder, etc.) have been filtered out. "
                "ZIP codes and city names are extracted when available."
            )

    # ============================================================
    # ========== â‘¡ Migration Direction (State-Based) =============
    # ============================================================

    st.markdown("### ğŸ§­ Migration Direction Over the Past 30 Days")

    if len(df_pred) < 3:
        st.warning("Not enough data to determine migration direction.")
    else:
        # Extract US state from location string
        def extract_state(loc):
            parts = str(loc).split(",")
            if len(parts) >= 2:
                s = parts[-1].strip()
                if len(s) == 2:  # e.g., IL, WI, TX
                    return s
            return None

        df_pred["state"] = df_pred["locName"].apply(extract_state)
        df_pred = df_pred.dropna(subset=["state"])

        # Average latitude per state
        state_lat = df_pred.groupby("state")["lat"].mean().to_dict()

        # Start & end of time series
        earliest_state = df_pred.iloc[0]["state"]
        latest_state = df_pred.iloc[-1]["state"]

        if earliest_state in state_lat and latest_state in state_lat:
            start_lat = state_lat[earliest_state]
            end_lat = state_lat[latest_state]
            lat_change = end_lat - start_lat

            # Determine migration direction (>1Â° â‰ˆ 111 km)
            if lat_change > 1.0:
                direction = f"â¬†ï¸ Northward Migration: {earliest_state} â†’ {latest_state}"
            elif lat_change < -1.0:
                direction = f"â¬‡ï¸ Southward Migration: {earliest_state} â†’ {latest_state}"
            else:
                direction = f"â¡ï¸ Minimal Movement: {earliest_state} â†’ {latest_state}"

            st.success(direction)
            st.write(f"Latitude change: {lat_change:.2f}Â°")
        else:
            st.warning("Unable to extract enough state information.")

else:
    st.info("ğŸ‘‰ Please search for a species first to generate heatmap data.")
